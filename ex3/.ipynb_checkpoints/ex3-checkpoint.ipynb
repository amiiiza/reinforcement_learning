{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 3 - Q Learning </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Cartpole </a>\n",
    "* <a href='#2.'> 3. Lunar Lander </a>\n",
    "* <a href='#4.'> 4. Submitting </a>\n",
    "* <a href='#4.1'> 4.1 Feedback </a>\n",
    "* <a href='#5'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing Q-Learning  (25 points) </a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Visualizing the Value Function (10 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 2.1</b> Analyzing the Value Function Heatmap (15 points) </a>\\\n",
    "<a href='#T3'><b>Student Task 3.</b> Investigating Initial Values (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 3.1</b> Analyzing Initial Values (5 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 3.2</b> Exploration (15 points) </a>\\\n",
    "<a href='#T4'><b>Student Task 4.</b> Using Q-Learning on the Lunar Lander Environment (5 points) </a>\\\n",
    "<a href='#Q4'><b>Student Question 4.1</b> Lunar Lander Performance (15 points) </a>\n",
    "\n",
    "**Total Points:** 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845acab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In this exercise, you'll be applying grid-based Q-learning to the **Cartpole** and **LunarLander** environments.\n",
    "\n",
    "## 1.1 Learning Objectives: <a id='1.1'></a>\n",
    "- To understand and develop intuition about Q-learning.\n",
    "- To understand simple exploration methods like GLIE and how exploration effects learning\n",
    "\n",
    "## 1.2 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "You don‚Äôt have to edit any other file other than ```ex3.ipynb``` to complete this exercise.\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                           # Config files for environments e.g. define the maximum number of steps in an episode.\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                          # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄCartPole-v1\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                  # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îÇ   logging.pkl            # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *q_table_e.pkl         # Contains qtable data for epsilon fixed\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *q_table_glie.pkl      # Contains qtable data for glie\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *e.png                 # Contains training performance plot for epsilon fixed\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *glie.png              # Contains training performance plot for glie\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄLunarLander-v2\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo\n",
    "‚îÇ   ‚îÇ   ‚îÇ   logging.pkl\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *q_table_e.pkl\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *q_table_glie.pkl\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *e.png                 \n",
    "‚îÇ   ‚îÇ   ‚îÇ   *glie.png              \n",
    "‚îÇ   ex3.ipynb                      # Main assignment file containing tasks <---------\n",
    "‚îÇ   setup.py                       # Contains setup function\n",
    "‚îÇ   utils.py                       # Contains useful functions \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c999cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Cartpole <a id='2.'></a>\n",
    "\n",
    "Recall the Cartpole environment from Exercise 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfe17b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing Q-Learning  (25 points) </h3> \n",
    "\n",
    "Implement Q-learning as presented in [Sutton and Barto, 2017, Section 6.5][1] for the Cartpole environment. Once you've successfully implemented the `get_action` and `update_q_value` functions,\n",
    "you can compare the two exploration methods:\n",
    "\n",
    "1. Use a constant value of $\\epsilon = 0.1$:\n",
    "    \n",
    "- Training:\n",
    "    ```python\n",
    "    # Training with constant epsilon 0.1\n",
    "    cfg_args = dict(epsilon=0.1, save_video=False)\n",
    "    train(cfg_path=Path().cwd() / 'cfg' / 'cartpole_v1.yaml', cfg_args=cfg_args)\n",
    "    ```\n",
    "- Testing:\n",
    "    ```python\n",
    "    # Testing with constant epsilon 0.1\n",
    "    cfg_args = dict(epsilon=0.1, save_video=True)\n",
    "    test(cfg_path=Path().cwd() / 'cfg' / 'cartpole_v1.yaml', cfg_args=cfg_args)\n",
    "    ```\n",
    "- Visualization: To record and visualize the agent's performance, you could set `save_video=True` and run the following:\n",
    "\n",
    "    ```python\n",
    "    Video(Path().cwd()/'results' / 'CartPole-v1' / 'video' / 'test' / 'ex3-episode-0.mp4', embed=True, html_attributes=\"loop autoplay\")\n",
    "    ```\n",
    "\n",
    "2. Implement **GLIE** (i.e., *Greedy in the Limit with Infinite Exploration*), which reduces the value of $\\epsilon$ over time. For **GLIE**, the goal is to reach $\\epsilon = 0.1$ after 20,000 episodes. Round the value of constant `glie_b` to the nearest integer. For details on the formula, refer to **[Lecture 3]**.\n",
    "    \n",
    "- Training (Ensure you've determined the correct value for `glie_b`):  \n",
    "    ```python\n",
    "    # Training with GLIE\n",
    "    cfg_args = dict(epsilon='glie', glie_b=<insert-correct-value>, save_video=False) # insert correct value for glie_b\n",
    "    train(cfg_path=Path().cwd() / 'cfg' / 'cartpole_v1.yaml', cfg_args=cfg_args) # < 30 mins\n",
    "    ```\n",
    "- Testing:\n",
    " \n",
    "    ```python\n",
    "    # Testing with GLIE\n",
    "    cfg_args = dict(epsilon='glie', save_video=True)\n",
    "    test(cfg_path=Path().cwd() / 'cfg' / 'cartpole_v1.yaml', cfg_args=cfg_args)\n",
    "    ```\n",
    "- Visualization: To record and visualize the agent's performance, you could set `save_video=True` and run the following:\n",
    "\n",
    "    ```python\n",
    "    Video(Path().cwd() / 'results' / 'CartPole-v1' / 'video' / 'test' / 'ex3-episode-0.mp4', embed=True, html_attributes=\"loop autoplay\")\n",
    "    ```\n",
    "    \n",
    "These videos will be stored in the **results** folder.\n",
    "\n",
    "**Note:** Before you proceed with the \"Training with GLIE\" method, ensure you have completed the \"Training with a constant epsilon\" steps. Since the video save path is the same, the results will be overwritten.\n",
    "\n",
    "---\n",
    "\n",
    "**Please attach the following files to your submission:**\n",
    "\n",
    "- Training performance plots:\n",
    "  - `task1_e.png`: This file represents the episode versus the smoothed episodic reward for the constant exploration rate.\n",
    "  - `task1_glie.png`: This file represents the episode versus the smoothed episodic reward for GLIE.\n",
    "\n",
    "- Q-table files:\n",
    "  - `task1_q_table_e.pkl`: Q-table for the constant exploration rate.\n",
    "  - `task1_q_table_glie.pkl`: Q-table for GLIE.\n",
    "\n",
    "Ensure all files are correctly named and included.\n",
    "\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/ep_reward.png\" width=\"600px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 1: The training performance plot should look similar to the one presented here when using GLIE  </figcaption>\n",
    "</figure>\n",
    "\n",
    "### References\n",
    "[1]: http://incompleteideas.net/book/RLbook2018.pdf\n",
    "[Sutton and Barto, 2017]: Sutton, Richard S., and Andrew G. Barto. \"Reinforcement Learning: An Introduction (in progress).\" London, England (2017).\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d6ce86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- Third-party Libraries ---\n",
    "import numpy as np  # Fundamental package for scientific computing\n",
    "from IPython.display import Video  # For displaying videos in the notebook\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Custom Modules ---\n",
    "from setup import setup  # Setup functions for training\n",
    "import utils as u  # Helper functions for the notebook\n",
    "\n",
    "# --- Configurations ---\n",
    "# Set working directory to 'results'\n",
    "work_dir = Path().cwd() / 'results'\n",
    "\n",
    "# Suppress DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91287ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_q_table(observation_space, action_dim, discr, bool_position=None, init_q=0.):\n",
    "    \"\"\"\n",
    "    Initialize a Q-table for the given observation space and actions.\n",
    "    \n",
    "    Parameters:\n",
    "    - observation_space: The state space of the environment.\n",
    "    - action_dim: The number of possible actions.\n",
    "    - discr: The number of divisions for discretization.\n",
    "    - bool_position: Indexes where the observation space has boolean values.\n",
    "    - init_q: Initial value for the Q-table.\n",
    "    \n",
    "    Returns:\n",
    "    - axis: A list of numpy arrays representing the discrete states. \n",
    "            Each array's length corresponds to either 'discr' or 2 (for boolean values).\n",
    "    - q_table: The initialized Q-table. Its shape is determined by [*axis dimensions, action_dim].\n",
    "               For example, if the axis dimensions were [10, 10] and action_dim was 3, \n",
    "               the q_table shape would be [10, 10, 3].\n",
    "    \"\"\"\n",
    "    high_values = observation_space.high\n",
    "    low_values = observation_space.low\n",
    "    axis = []\n",
    "    for idx, (low_val, high_val) in enumerate(zip(low_values, high_values)):\n",
    "        # here to avoid inf boundary, we truncate the value to [-4, 4] \n",
    "        if low_val < -1e10: low_val = -4\n",
    "        if high_val > 1e10: high_val = 4\n",
    "        \n",
    "        if (bool_position is not None) and (idx in bool_position):\n",
    "            axis.append(np.linspace(low_val, high_val, 2, dtype=np.float32)) # for boolean, we only have two values: 1., 0.\n",
    "        else:\n",
    "            axis.append(np.linspace(low_val, high_val, discr, dtype=np.float32))\n",
    "    _shape = [ax.shape[0] for ax in axis] + [action_dim]\n",
    "    q_table = np.zeros(_shape) + init_q\n",
    "    return axis, q_table\n",
    "\n",
    "def get_table_idx(state, axis):\n",
    "    \"\"\"\n",
    "    Give a state, discretize it, and return the index in each dimension (axis). \n",
    "    With the returned index, you can access q(s,.) with q_table[idx].\n",
    "    \n",
    "    Parameters:\n",
    "    - state: The state to be discretized. \n",
    "    - axis: The discrete state space as a list of numpy arrays. \n",
    "    \n",
    "    Returns:\n",
    "    - idx: A tuple representing the index of the discretized state in the Q-table. \n",
    "           Its shape matches the dimensions of the provided 'axis'.\n",
    "    \"\"\"\n",
    "    def _get_ax_idx(ax, value):\n",
    "        return np.argmin(np.abs(ax - value))\n",
    "    return tuple([_get_ax_idx(ax, value) for ax, value in zip(axis, state)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b139b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_action(state, q_axis, q_table, epsilon=0.0):\n",
    "    \"\"\"\n",
    "    Determine the action to take based on epsilon-greedy strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    - state: The current state of the agent.\n",
    "    - q_axis: The discrete state space as a list of numpy arrays.\n",
    "    - q_table: The Q-table with learned values.\n",
    "    - epsilon: The probability with which a random action is taken (exploration). \n",
    "               Default is 0 (purely greedy).\n",
    "    \n",
    "    Returns:\n",
    "    - action: The chosen action, either greedily or randomly based on epsilon.\n",
    "    \"\"\"\n",
    "    # if epsilon == 0.0, the policy will be greedy -- always choose the best action\n",
    "\n",
    "    # TODO: Task 1, implement epsilon-greedy\n",
    "    # 1. Discretize the given state by using the get_table_idx(state, axis) function\n",
    "    # 2. Use the discretized state's index to access the Q-table\n",
    "    # 3. Generate a random number between 0 and 1.\n",
    "    # 4. Compare this number to `epsilon`\n",
    "    # 5. If the random number is less than `epsilon`, choose a random action.\n",
    "    # 6. Otherwise, choose the best action (greedy action)\n",
    "    ########## Your code starts here ##########\n",
    "        \n",
    "    ########## Your code ends here #########\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_q_value(old_state, action, new_state, gamma, reward, done, alpha, q_axis, q_table):\n",
    "    \"\"\"\n",
    "    Update the Q-value for a state-action pair based on the Q-learning update rule.\n",
    "    \n",
    "    Parameters:\n",
    "    - old_state: The original state before taking the action.\n",
    "    - action: The action taken.\n",
    "    - new_state: The state after taking the action.\n",
    "    - gamma: Discount factor.\n",
    "    - reward: Immediate reward for taking the action.\n",
    "    - done: Boolean indicating if the episode is done.\n",
    "    - alpha: Learning rate.\n",
    "    - q_axis: The discrete state space as a list of numpy arrays.\n",
    "    - q_table: The Q-table with learned values.\n",
    "    \n",
    "    Returns:\n",
    "    - q_table: Updated Q-table.\n",
    "    \"\"\"\n",
    "\n",
    "    # q_table contains discretized state-action space (16, 16, 16, 16, 2) values\n",
    "    # Each observation is discritized 16 times and the last dimension are the two\n",
    "    # actions that can be taken. \n",
    "    \n",
    "    # TODO: Task 1, update q value\n",
    "    # 1. Retrieve Old Q-Value\n",
    "    # 2. Determine the Next Action\n",
    "    # 3. Retrieve New Q-Value\n",
    "    #     If the episode isn't done, get the Q-value for the new_state and next_action.\n",
    "    #     If the episode is done, set the new Q-value to zero.\n",
    "    # 4. Compute the Updated Q-Value\n",
    "    # 5. Update the Q-Table \n",
    "    # 6. Return the Updated Q-Table\n",
    "\n",
    "    old_table_idx = get_table_idx(old_state, q_axis) # idx of q(s_old, *)\n",
    "    new_table_idx = get_table_idx(new_state, q_axis) # idx of q(s_new, *)  \n",
    "    ########## Your code starts here ##########\n",
    "    \n",
    "    ########### Your code ends here ########## \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62297b95-5f55-4dcc-827b-ff984ae99bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_data(training_data, save_path):\n",
    "    df = pd.DataFrame(training_data)\n",
    "    plt.figure(figsize=(5, 3.5))\n",
    "    sns.lineplot(data=df, x='episode', y='ep_reward_avg')\n",
    "    plt.title('Training Performance')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def train(cfg_path, cfg_args={}):\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "    performance_data = []\n",
    "\n",
    "    # init q_table with zeros\n",
    "    q_axis, q_table = init_q_table(env.observation_space, \n",
    "        env.action_space.n, cfg.discr, bool_position=cfg.bool_position, init_q=cfg.initial_q)\n",
    "\n",
    "    # begin training and testing\n",
    "    ep_reward_deque = deque([], maxlen=500)  # used to calculate the smoothed (avg over recent 500 episodes) ep_reward\n",
    "    for ep in range(cfg.train_episodes + 1):\n",
    "        # set epsilon value\n",
    "        if cfg.epsilon == 'glie':\n",
    "            epsilon = cfg.glie_b / (cfg.glie_b + ep)  \n",
    "        elif isinstance(cfg.epsilon, (int, float)):\n",
    "            epsilon = cfg.epsilon\n",
    "        else: \n",
    "            raise ValueError\n",
    "        \n",
    "        (state, _), done, ep_reward, timesteps = env.reset(seed=cfg.seed), False, 0, 0\n",
    "        while not done:\n",
    "            action = get_action(state, q_axis, q_table, epsilon=epsilon) \n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "            q_table = update_q_value(state, action, new_state, cfg.gamma, reward, done, cfg.alpha,\n",
    "                                        q_axis, q_table)\n",
    "    \n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            timesteps += 1\n",
    "            if timesteps >= env.spec.max_episode_steps:\n",
    "                done = True\n",
    "                \n",
    "        ep_reward_deque.append(ep_reward)\n",
    "        info = {\n",
    "            'episode': ep,\n",
    "            'epsilon': epsilon,\n",
    "            'ep_reward': ep_reward,\n",
    "            'timesteps': timesteps,\n",
    "            'ep_reward_avg': np.mean(list(ep_reward_deque))\n",
    "        }\n",
    "\n",
    "        performance_data.append(info)\n",
    "        \n",
    "\n",
    "        if (not cfg.silent) and (ep % 500 == 0): print(info)\n",
    "    \n",
    "    work_dir = Path().cwd()/'results'/f'{cfg.env_name}'\n",
    "      \n",
    "    # save the q-value table and q_axis\n",
    "    file_pre = f\"{cfg.task_no}_\" if hasattr(cfg, 'task_no') else \"\"\n",
    "    file = file_pre + 'q_table_glie.pkl' if cfg.epsilon == 'glie' else file_pre + 'q_table_e.pkl'\n",
    "    u.save_object({'q_table': q_table, 'axis': q_axis},\n",
    "                        work_dir/file)\n",
    "    png_file = file_pre + '_glie.png' if cfg.epsilon == 'glie' else file_pre + '_e.png'\n",
    "    plot_training_data(performance_data, work_dir/png_file)\n",
    "    print('Training done!')\n",
    "\n",
    "    \n",
    "def test(cfg_path, cfg_args={'testing':True}):\n",
    "    # test under random seed for each episode\n",
    "    seed = np.random.randint(0, 10000)\n",
    "    \n",
    "    cfg_args.update({'testing':True})\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "    \n",
    "    work_dir = Path().cwd()/'results'/f'{cfg.env_name}'\n",
    "    \n",
    "    # load q_table\n",
    "    file_pre = f\"{cfg.task_no}_\" if hasattr(cfg, 'task_no') else \"\"\n",
    "    file = file_pre + 'q_table_glie.pkl' if cfg.epsilon == 'glie' else file_pre + 'q_table_e.pkl'\n",
    "    data = u.load_object(work_dir/file)\n",
    "    q_axis, q_table = data['axis'], data['q_table']\n",
    "\n",
    "    # begin testing\n",
    "    avg_ep_reward = 0\n",
    "    for ep in range(cfg.test_episodes):\n",
    "        (state, _), done, ep_reward, timesteps = env.reset(seed=seed + ep), False, 0, 0\n",
    "        while not done:\n",
    "            action = get_action(state, q_axis, q_table, epsilon=0.0) # be greedy during testing\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            timesteps += 1\n",
    "            if timesteps >= env.spec.max_episode_steps:\n",
    "                done = True\n",
    "        \n",
    "        info = {\n",
    "            'test_episode': ep,\n",
    "            'test_ep_reward': ep_reward,\n",
    "            'timesteps': timesteps,\n",
    "        }\n",
    "        avg_ep_reward += ep_reward\n",
    "   \n",
    "        if not cfg.silent: \n",
    "            print(info)\n",
    "    \n",
    "    print(f'Testing done! Average test reward is {avg_ep_reward / cfg.test_episodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace84a6-031e-4b0d-9d99-511af93202eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training with constant epsilon 0.1\n",
    "cfg_args = dict(epsilon=0.1, save_video=False)\n",
    "train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args) # < 30 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d603c2a-5582-4a7b-a04b-7b6ba03fc89d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing with constant epsilon 0.1\n",
    "cfg_args = dict(epsilon=0.1, save_video=True)\n",
    "test(cfg_path=Path().cwd() / 'cfg' / 'cartpole_v1.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6dd079-2502-4c2c-bd8e-0ac321b5826d",
   "metadata": {},
   "source": [
    "To visualize how the agent acts within the environment, execute the cell below. **Modify the path** to select the episode you'd like to observe. Remember, by default, videos are saved every 500 episodes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b9226-601e-4ba3-ae3b-05f5ea36d3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing the results with constant epsilon 0.1\n",
    "Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex3-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f187979-feb2-4833-a11a-68cfacc62080",
   "metadata": {},
   "source": [
    "\n",
    "**Important Reminder:** Before initiating the \"Training with GLIE\" process, make sure you've fully executed the steps under \"Training with a constant epsilon\". Be cautious, as the video save path remains identical; thus, any previous results will be overwritten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5da29e-f3f3-4b23-b879-a7786eb25c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training with GLIE\n",
    "cfg_args = dict(epsilon='glie', glie_b=<insert-correct-value>, save_video=False) # insert correct value for glie_b\n",
    "train(cfg_path=Path().cwd() / 'cfg' / 'cartpole_v1.yaml', cfg_args=cfg_args) # < 30 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77aea8d-2c8c-4e8f-baec-a3fd68fead5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing with GLIE\n",
    "cfg_args = dict(epsilon='glie', save_video=True)\n",
    "test(cfg_path=Path().cwd() / 'cfg' / 'cartpole_v1.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec32a0-acec-46fb-80cb-72bf8b6c5843",
   "metadata": {},
   "source": [
    "Just as in the \"visualize the results with constant epsilon 0.1\" section, **modify the path** to select the episode you'd like to observe. Remember, videos are saved every 500 episodes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6204544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing the results with GLIE\n",
    "Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex3-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e2e61",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Visualizing the Value Function (10 points) </h3> \n",
    "\n",
    "Use your Q-function values estimated with GLIE to calculate the optimal value function of each state. Complete the ```display_heatmap_vf()``` function below to plot the heatmap of the value function in terms of $x$ and $\\theta$, such that $\\theta$ is on horizontal and $x$ is on vertical axis. For plotting, average the values over $\\dot{x}$ and $\\dot{\\theta}$. Attach the heatmap in your report.\n",
    "\n",
    "**Hint:** For plotting the heatmap you can use Matplotlib-pyplot: ```pyplot.imshow(values_array)``` or Seaborn: ```seaborn.heatmap(values_array)```\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7567c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "from pathlib import Path    # For handling filesystem paths in a more intuitive way\n",
    "\n",
    "# --- Third-party Libraries ---\n",
    "import matplotlib.pyplot as plt  # For creating static, interactive, and animated visualizations\n",
    "import seaborn as sns            # A data visualization library based on matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d44fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_heatmap_vf(env_name=\"CartPole-v1\", q_table_name='task1_q_table_glie.pkl'):\n",
    "    \"\"\"\n",
    "    Displays a heatmap of the value function for a given environment and Q-table.\n",
    "\n",
    "    Input:\n",
    "    - env_name (str): Name of the environment, default is \"CartPole-v1\".\n",
    "    - q_table_name (str): Name of the saved Q-table file, default is 'q_table_glie.pkl'.\n",
    "\n",
    "    Output:\n",
    "    - Visual display of the heatmap.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the Q-value array\n",
    "    work_dir = Path().cwd()/'results'/f'{env_name}'\n",
    "    \n",
    "    data = u.load_object(work_dir/q_table_name) # load q_table \n",
    "    q_axis, q_table = data['axis'], data['q_table'] \n",
    "    x_axis, th_axis = q_axis[0], q_axis[2]  # get the axis for x and \\theta\n",
    "\n",
    "    discr = q_table.shape[0] # get the number of discretized bins for a state variable in the Q-table\n",
    "\n",
    "    # TODO: Task 2 Plot the heatmap of the value function\n",
    "    # 1. Compute the maximum Q-value for each state, which gives the value function of that state\n",
    "    # 2. Uses Seaborn and Matplotlib to display the heatmap.  \n",
    "    ########## Your code begins here. ##########\n",
    "\n",
    "    ########## Your code ends here. ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4bd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visual display of the heatmap of the value function\n",
    "display_heatmap_vf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba1c47",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.1</b> Analyzing the Value Function Heatmap (15 points) </h3> \n",
    "\n",
    "What do you think the heatmap would have looked like:\n",
    "1) before the training?\n",
    "2) after a single episode?\n",
    "3) halfway through the training?\n",
    "\n",
    "***Justify why for all the cases. Attaching the plots is not required.*** \n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02200ecc",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb7b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3.</b> Investigating Initial Values (10 points) </h3> \n",
    "\n",
    "Set $\\epsilon$ to zero, effectively making the policy greedy w.r.t. current Q-value estimates. Run the training again while\n",
    "1) keeping the initial estimates of the Q function at 0,\n",
    "2) setting the initial estimates of the Q function to 50 for all states and actions.\n",
    "\n",
    "You can change the initial Q value estimates by passing ```initial_q=<value>``` in the following cells. \n",
    "\n",
    "**Attach training performance plots of both initializations in your report.**\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf3dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Q function with a value of 0 and begin training\n",
    "cfg_args = dict(epsilon=0, initial_q=<insert-q-value>, task_no='task3.1') # set the initial estimates of the Q function to 0\n",
    "train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee238f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visual display of the heatmap of the value function\n",
    "display_heatmap_vf('CartPole-v1', 'task3.1_q_table_e.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc746ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Q function with a value of 50 and begin training\n",
    "cfg_args = dict(epsilon=0, initial_q=<insert-q-value>, task_no='task3.2') # set the initial estimates of the Q function to 50\n",
    "train(cfg_path=Path().cwd() / 'cfg' / 'cartpole_v1.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed3aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visual display of the heatmap of the value function\n",
    "display_heatmap_vf('CartPole-v1', 'task3.2_q_table_e.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea9f8b",
   "metadata": {},
   "source": [
    "Based on the results you observed in Task 3, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97da87",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 3.1</b> Analyzing Initial Values (5 points) </h3> \n",
    "\n",
    "In which case does the model perform better?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fcbcf9",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a200626",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 3.2</b> Exploration (15 points) </h3> \n",
    "\n",
    "Why is this the case, and how does the initialization of Q values affect exploration?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b77508",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176d62f",
   "metadata": {},
   "source": [
    "## 3. Lunar lander <a id='3.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95724c44",
   "metadata": {},
   "source": [
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/lunar_lander.png\" width=\"600px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2: The Lunar lander environment. </figcaption>\n",
    "</figure>\n",
    "\n",
    "The ***Lunar lander*** environment is shown in Figure 2. The goal is to make the lunar lander land on the ground between two flag poles. The agent receives a positive reward for moving towards the landing pad, landing, etc. A negative reward is given for firing the main engine (more fuel-efficient policies are better) and for crashing. Four actions are available: firing the left/right/main engines, or doing nothing (free fall). The observation vector consists of 6 continuous and 2 discrete values:\n",
    "\n",
    "$$\n",
    "o=\\left(\\begin{array}{llllllll}\n",
    "x & y & \\dot{x} & \\dot{y} & \\theta & \\dot{\\theta} & c_l & c_r\n",
    "\\end{array}\\right)^T,\n",
    "$$\n",
    "\n",
    "where $x$ and $y$ are the coordinates of the lander, $\\dot{x}$ and $\\dot{y}$ its velocities, $\\theta$ represents the rotation angle and $\\dot{\\theta}$ the angular velocity of the lander. Two discrete values $c_l$ and $c_r$  indicate whether the lander‚Äôs legs are in contact with the ground (0 or 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636f10e",
   "metadata": {},
   "source": [
    "<a id='T4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 4.</b> Using Q-Learning on the Lunar Lander Environment (5 points) </h3> \n",
    "\n",
    "1) Run the training for Lunar Lander environment by executing the following:\n",
    "```python \n",
    "cfg_args = dict(epsilon='glie', glie_b=<insert-glie-from-task1>, save_video=False, discr=12) # insert correct value for glie_b\n",
    "train(cfg_path=Path().cwd()/'cfg'/'lunarlander_v2.yaml', cfg_args=cfg_args)\n",
    "```\n",
    "2) Run it for 20000 episodes (which was enough for the Cartpole to learn). **Attach the training performance plot in your submission.**\n",
    "\n",
    "**Heads Up:** The generated file `task4_q_table.pkl` is sizeable, approximately 2 GB. Ensure you **delete** it before submitting.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43485e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training on the Lunar Lander Environment\n",
    "cfg_args = dict(epsilon='glie', glie_b=<insert-glie-from-task1>, save_video=False, discr=12) # insert correct value for glie_b\n",
    "train(cfg_path=Path().cwd()/'cfg'/'lunarlander_v2.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4481f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_args = dict(epsilon='glie', save_video=True, test_episodes=5)\n",
    "test(cfg_path=Path().cwd()/'cfg'/'lunarlander_v2.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f33113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd()/'results'/'LunarLander-v2'/'video'/'test'/'ex3-episode-4.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd6a58",
   "metadata": {},
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.1</b> Lunar Lander Performance (15 points) </h3> \n",
    "\n",
    "Does the lander learn to land between the flag poles? Why/why not?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efbd2d",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c516e-fa77-4fdb-9ff7-9b85c74555ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Submitting <a id='4.'></a>\n",
    "Ensure all tasks and questions (in ```ex3.ipynb```) are answered and the relevant plots are recorded in the relevant places. Details about attaching images and figures can be found below. The relevant graphs to be included for this assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `task1_e.png`: Cartpole, constant value of $\\epsilon$ training performance plots in terms of episode and smoothed episodic reward\n",
    "  - `task1_glie.png`: Cartpole, GLIE training performance plots in terms of episode and smoothed episodic reward\n",
    "  - `task3.1_e.png`: Cartpole, training performance plots of the initial estimates of the Q function at 0 \n",
    "  - `task3.2_e.png`: Cartpole, training performance plots of the initial estimates of the Q function at 50\n",
    "  - `task4_glie.png`: Luner Lander, training performance plots in terms of episode and smoothed episodic reward\n",
    "- Q-table files:\n",
    "  - `task1_q_table_e.pkl`: Cartpole, Q-table for the constant exploration rate.\n",
    "  - `task1_q_table_glie.pkl`: Cartpole, Q-table for GLIE.\n",
    "\n",
    "\n",
    "Ensure the correct model files and plots are saved:\n",
    "- ```results/CartPole-v1/task1_e.png``` From Task 1\n",
    "- ```results/CartPole-v1/task1_glie.png``` From Task 1\n",
    "- ```results/CartPole-v1/task1_q_table_e.pkl``` From Task 1\n",
    "- ```results/CartPole-v1/task1_q_table_glie.pkl``` From Task 1\n",
    "- ```results/CartPole-v1/task3.1_e.png``` From Task 3\n",
    "- ```results/CartPole-v1/task3.2_e.png``` From Task 3\n",
    "- ```results/LunarLander-v2/task4_glie.png``` From Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb3823",
   "metadata": {},
   "source": [
    "## 4.1 Feedback <a id='4.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19aa8d-6ce6-4688-b6f8-db0ae3caa596",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c8370-ff84-4dc9-aab8-596782c55d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d254d1-ad16-4c10-87a6-72afb79d49fe",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58193777-64de-4d7c-be84-249d34e1c027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Student Task 1. Implementing Q-Learning (25 points)\n",
    "T2 = None   # Student Task 2. Visualizing the Value Function (10 points)\n",
    "Q2_1 = None # Student Question 2.1 Analyzing the Value Function Heatmap (15 points)\n",
    "T3 = None   # Student Task 3. Investigating Initial Values (10 points)\n",
    "Q3_1 = None # Student Question 3.1 Analyzing Initial Values (5 points)\n",
    "Q3_2 = None # Student Question 3.2 Exploration (15 points)\n",
    "T4 = None   # Student Task 4. Using Q-Learning on the Lunar Lander Environment (5 points)\n",
    "Q4_1 = None # Student Question 4.1 Lunar Lander Performance (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af146c4d-9475-4c0b-8b43-823fc345a584",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a460a-eb5b-45e4-a091-7153416dbd43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Student Task 1. Implementing Q-Learning (25 points)\n",
    "T2 = None   # Student Task 2. Visualizing the Value Function (10 points)\n",
    "Q2_1 = None # Student Question 2.1 Analyzing the Value Function Heatmap (15 points)\n",
    "T3 = None   # Student Task 3. Investigating Initial Values (10 points)\n",
    "Q3_1 = None # Student Question 3.1 Analyzing Initial Values (5 points)\n",
    "Q3_2 = None # Student Question 3.2 Exploration (15 points)\n",
    "T4 = None   # Student Task 4. Using Q-Learning on the Lunar Lander Environment (5 points)\n",
    "Q4_1 = None # Student Question 4.1 Lunar Lander Performance (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4ebe0-4012-42fd-90ae-2c8e386afa4f",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "And other feedback you think is worth including. Type in the box below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c9543-3652-4311-ad4d-6b6a087e5587",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9618d7-d671-4951-b9c6-523a72362e68",
   "metadata": {},
   "source": [
    "## References <a id='5'></a>\n",
    "[1]: http://incompleteideas.net/book/RLbook2018.pdf\n",
    "[Sutton and Barto, 2017]: Sutton, Richard S., and Andrew G. Barto. \"Reinforcement Learning: An Introduction (in progress).\" London, England (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60309fa-2971-4d5b-afa9-2e352e7834b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
